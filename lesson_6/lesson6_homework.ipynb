{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T07:35:35.642874Z",
     "start_time": "2024-11-15T07:35:35.636796Z"
    }
   },
   "source": [
    "# make sure you have the latest version of sigmoid_check installed by running pip install sigmoid_check --upgrade\n",
    "from sigmoid_check.excalibur import *"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:54.221956Z",
     "start_time": "2024-11-11T14:29:54.186302Z"
    }
   },
   "source": [
    "# Extract DataFrame `df_social_media_metrics` subset for rows where 'Likes' exceed 'Shares' and store it in `highly_liked_posts`.\n",
    "@check_pandas_101\n",
    "def pandas_101(df_social_media_metrics):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_social_media_metrics)\n",
    "    highly_liked_posts = df_social_media_metrics[df_social_media_metrics.Likes > df_social_media_metrics.Shares]\n",
    "    print(highly_liked_posts)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"highly_liked_posts\": highly_liked_posts}\n",
    "\n",
    "pandas_101()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Likes  Shares\n",
      "0    100     150\n",
      "1    200     150\n",
      "2    300     250\n",
      "   Likes  Shares\n",
      "1    200     150\n",
      "2    300     250\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:54.477440Z",
     "start_time": "2024-11-11T14:29:54.428206Z"
    }
   },
   "source": [
    "# Use DataFrame `df_product_info` to group by 'Category', computing the minimum 'Price' for each category, storing result as `min_price_by_category`.\n",
    "@check_pandas_102\n",
    "def pandas_102(df_product_info):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_product_info)\n",
    "    min_price_by_category = df_product_info.groupby('Category')['Price'].min()\n",
    "    print(min_price_by_category)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"min_price_by_category\": min_price_by_category}\n",
    "\n",
    "pandas_102()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Price\n",
      "0        A    100\n",
      "1        B    200\n",
      "2        A    300\n",
      "3        B    400\n",
      "Category\n",
      "A    100\n",
      "B    200\n",
      "Name: Price, dtype: int64\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:54.603565Z",
     "start_time": "2024-11-11T14:29:54.568227Z"
    }
   },
   "source": [
    "# Apply datetime filtering on `df_online_sessions` to store sessions in the month of January 2023 in variable `january_sessions` based on 'SessionStart'.\n",
    "@check_pandas_103\n",
    "def pandas_103(df_online_sessions):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_online_sessions)\n",
    "    january_sessions = df_online_sessions[df_online_sessions['SessionStart'].dt.month == 1]\n",
    "    print(january_sessions)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"january_sessions\": january_sessions}\n",
    "\n",
    "pandas_103()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SessionID SessionStart\n",
      "0          1   2023-01-01\n",
      "1          2   2023-01-08\n",
      "2          3   2023-01-15\n",
      "3          4   2023-01-22\n",
      "4          5   2023-01-29\n",
      "5          6   2023-02-05\n",
      "   SessionID SessionStart\n",
      "0          1   2023-01-01\n",
      "1          2   2023-01-08\n",
      "2          3   2023-01-15\n",
      "3          4   2023-01-22\n",
      "4          5   2023-01-29\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:54.910787Z",
     "start_time": "2024-11-11T14:29:54.879827Z"
    }
   },
   "source": [
    "# Use DataFrame `df_employee_entries` and fill NA in 'EntryTime' with '08:00 AM', storing the result in `filled_employee_entries`.\n",
    "@check_pandas_104\n",
    "def pandas_104(df_employee_entries):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_employee_entries)\n",
    "    filled_employee_entries = df_employee_entries.fillna('08:00 AM')\n",
    "    print(filled_employee_entries)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filled_employee_entries\": filled_employee_entries}\n",
    "\n",
    "pandas_104()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeID EntryTime\n",
      "0           1  09:00 AM\n",
      "1           2       NaN\n",
      "   EmployeeID EntryTime\n",
      "0           1  09:00 AM\n",
      "1           2  08:00 AM\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:55.034493Z",
     "start_time": "2024-11-11T14:29:55.009279Z"
    }
   },
   "source": [
    "# Drop all columns with any NA values in DataFrame `df_transaction_records`, storing cleaned version as `non_na_transactions`.\n",
    "@check_pandas_105\n",
    "def pandas_105(df_transaction_records):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_transaction_records)\n",
    "    non_na_transactions = df_transaction_records.dropna(axis=1)\n",
    "    print(non_na_transactions)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"non_na_transactions\": non_na_transactions}\n",
    "\n",
    "pandas_105()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  Amount\n",
      "0              1   100.0\n",
      "1              2     NaN\n",
      "   TransactionID\n",
      "0              1\n",
      "1              2\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:55.182333Z",
     "start_time": "2024-11-11T14:29:55.169152Z"
    }
   },
   "source": [
    "# Create a Series `series_ascending` from a list of numbers [1, 2, 3, 4, 5], using these values as the indices as well.\n",
    "@check_pandas_106\n",
    "def pandas_106():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    series_ascending = pd.Series(data, index=data)\n",
    "    print(series_ascending)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"series_ascending\": series_ascending}\n",
    "\n",
    "pandas_106()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    4\n",
      "5    5\n",
      "dtype: int64\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:55.499456Z",
     "start_time": "2024-11-11T14:29:55.467408Z"
    }
   },
   "source": [
    "# Use the query method on DataFrame `df_financial_audit` to extract entries with 'Audit' == 'Complete' and 'Amount' > 10000, saving it as `completed_audits`.\n",
    "@check_pandas_107\n",
    "def pandas_107(df_financial_audit):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_financial_audit)\n",
    "    completed_audits = df_financial_audit.query(\"Audit == 'Complete' and Amount > 10000\")\n",
    "    print(completed_audits)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"completed_audits\": completed_audits}\n",
    "\n",
    "pandas_107()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Audit  Amount\n",
      "0    Complete   20000\n",
      "1  Incomplete    5000\n",
      "      Audit  Amount\n",
      "0  Complete   20000\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:55.695679Z",
     "start_time": "2024-11-11T14:29:55.679042Z"
    }
   },
   "source": [
    "# Create a custom `percent_round` function and apply it to round the 'Progress' column in DataFrame `df_student_projects` to the nearest ten, storing result as `rounded_progress`.\n",
    "@check_pandas_108\n",
    "def pandas_108(df_student_projects):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_student_projects)\n",
    "    def percent_round(value):\n",
    "        return round(value / 10) * 10\n",
    "    \n",
    "    df_student_projects['Progress'] = df_student_projects['Progress'].apply(percent_round)\n",
    "    rounded_progress = df_student_projects.copy()\n",
    "    print(rounded_progress)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"rounded_progress\": rounded_progress}\n",
    "\n",
    "pandas_108()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Project  Progress\n",
      "0       A        30\n",
      "1       B        80\n",
      "  Project  Progress\n",
      "0       A        30\n",
      "1       B        80\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:55.787726Z",
     "start_time": "2024-11-11T14:29:55.768537Z"
    }
   },
   "source": [
    "# Extract the elements starting from the 10th index to the 20th index from `df_user_activity`, storing the result in `sampled_user_activity`.\n",
    "@check_pandas_109\n",
    "def pandas_109(df_user_activity):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_user_activity)\n",
    "    sampled_user_activity = df_user_activity.iloc[10:20]\n",
    "    print(sampled_user_activity)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sampled_user_activity\": sampled_user_activity}\n",
    "\n",
    "pandas_109()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     UserID Activity\n",
      "0         0    Login\n",
      "1         1   Logout\n",
      "2         2    Login\n",
      "3         3   Logout\n",
      "4         4    Login\n",
      "..      ...      ...\n",
      "995     995   Logout\n",
      "996     996    Login\n",
      "997     997   Logout\n",
      "998     998    Login\n",
      "999     999   Logout\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "    UserID Activity\n",
      "10      10    Login\n",
      "11      11   Logout\n",
      "12      12    Login\n",
      "13      13   Logout\n",
      "14      14    Login\n",
      "15      15   Logout\n",
      "16      16    Login\n",
      "17      17   Logout\n",
      "18      18    Login\n",
      "19      19   Logout\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:56.002696Z",
     "start_time": "2024-11-11T14:29:55.978870Z"
    }
   },
   "source": [
    "# Use advanced indexing to set all negative values in DataFrame `df_temperature_fluctuations` to zero, storing corrected DataFrame as `non_negative_temperatures`.\n",
    "@check_pandas_110\n",
    "def pandas_110(df_temperature_fluctuations):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_temperature_fluctuations)\n",
    "    non_negative_temperatures = df_temperature_fluctuations[df_temperature_fluctuations > 0].fillna(0).astype(int)\n",
    "    print(non_negative_temperatures)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"non_negative_temperatures\": non_negative_temperatures}\n",
    "\n",
    "pandas_110()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature\n",
      "0           10\n",
      "1           -5\n",
      "2           20\n",
      "3          -10\n",
      "   Temperature\n",
      "0           10\n",
      "1            0\n",
      "2           20\n",
      "3            0\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:56.168100Z",
     "start_time": "2024-11-11T14:29:56.151819Z"
    }
   },
   "source": [
    "# With DataFrame `df_fleet_inventory`, allocate memory efficiency by converting 'Year' column to category, saving the result as `efficient_fleet_inventory`.\n",
    "@check_pandas_111\n",
    "def pandas_111(df_fleet_inventory):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_fleet_inventory)\n",
    "    df_fleet_inventory['Year'] = df_fleet_inventory['Year'].astype('category')\n",
    "    efficient_fleet_inventory = df_fleet_inventory.copy()\n",
    "    print(efficient_fleet_inventory)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"efficient_fleet_inventory\": efficient_fleet_inventory}\n",
    "\n",
    "pandas_111()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year\n",
      "0  2020\n",
      "1  2021\n",
      "2  2020\n",
      "3  2021\n",
      "   Year\n",
      "0  2020\n",
      "1  2021\n",
      "2  2020\n",
      "3  2021\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:56.345042Z",
     "start_time": "2024-11-11T14:29:56.319771Z"
    }
   },
   "source": [
    "# Use DataFrame `df_respiratory_data` to calculate cumulative maximum of 'Pulse' within groups of 'PatientID', storing it as `grouped_cumulative_max`.\n",
    "@check_pandas_112\n",
    "def pandas_112(df_respiratory_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_respiratory_data)\n",
    "    grouped_cumulative_max = df_respiratory_data.groupby('PatientID')['Pulse'].cummax()\n",
    "    print(grouped_cumulative_max)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"grouped_cumulative_max\": grouped_cumulative_max}\n",
    "\n",
    "pandas_112()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PatientID  Pulse\n",
      "0          1    100\n",
      "1          1    110\n",
      "2          2    120\n",
      "3          2    130\n",
      "0    100\n",
      "1    110\n",
      "2    120\n",
      "3    130\n",
      "Name: Pulse, dtype: int64\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:56.523281Z",
     "start_time": "2024-11-11T14:29:56.484687Z"
    }
   },
   "source": [
    "# Group DataFrame `df_weather_statistics` and apply aggregation to find both 'mean' and 'std' of 'Temperature', storing multi-aggregate result as `weather_stats`.\n",
    "@check_pandas_113\n",
    "def pandas_113(df_weather_statistics):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_weather_statistics)\n",
    "    weather_stats = df_weather_statistics.groupby('City')['Temperature'].agg(['mean', 'std'])\n",
    "    print(weather_stats)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"weather_stats\": weather_stats}\n",
    "\n",
    "pandas_113()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  City  Temperature\n",
      "0    A           10\n",
      "1    A           20\n",
      "2    B           30\n",
      "3    B           40\n",
      "      mean       std\n",
      "City                \n",
      "A     15.0  7.071068\n",
      "B     35.0  7.071068\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:36:35.863360Z",
     "start_time": "2024-11-11T14:36:35.847503Z"
    }
   },
   "source": [
    "# Create a hierarchical index on DataFrame `df_multilayer_data` using [('Region', 'State')], saving the result as `hierarchical_multilayer`.\n",
    "@check_pandas_114\n",
    "def pandas_114(df_multilayer_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_multilayer_data)\n",
    "    df_multilayer_data.set_index(['Region', 'State'], inplace=True)\n",
    "    hierarchical_multilayer = df_multilayer_data.copy()\n",
    "    print(hierarchical_multilayer)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"hierarchical_multilayer\": hierarchical_multilayer}\n",
    "\n",
    "pandas_114()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region State\n",
      "0   East    NY\n",
      "1   East    NJ\n",
      "2   West    CA\n",
      "3   West    WA\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [(East, NY), (East, NJ), (West, CA), (West, WA)]\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:56.842854Z",
     "start_time": "2024-11-11T14:29:56.818616Z"
    }
   },
   "source": [
    "# Calculate the exponential moving average with a span of 10 on the 'Close' column in DataFrame `df_market_activity`, storing to `ema_market_activity`.\n",
    "@check_pandas_115\n",
    "def pandas_115(df_market_activity):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_market_activity)\n",
    "    ema_market_activity = df_market_activity['Close'].ewm(span=10).mean()\n",
    "    print(ema_market_activity)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"ema_market_activity\": ema_market_activity}\n",
    "\n",
    "pandas_115()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Close\n",
      "0 2022-01-01    100\n",
      "1 2022-01-02    110\n",
      "2 2022-01-03    120\n",
      "3 2022-01-04    130\n",
      "4 2022-01-05    140\n",
      "0    100.000000\n",
      "1    105.500000\n",
      "2    111.328904\n",
      "3    117.480198\n",
      "4    123.945021\n",
      "Name: Close, dtype: float64\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:36:55.079043Z",
     "start_time": "2024-11-11T14:36:55.064691Z"
    }
   },
   "source": [
    "# Create DataFrame `df_temperature_readings` for two sensors over the year using monthly datetime index, with lineaerly increasing temperature values from 20 to 30 for Sensor1 and 15 to 25 for Sensor2.\n",
    "@check_pandas_116\n",
    "def pandas_116():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    date = pd.date_range(start='2023-01-01', end='2023-12-31', freq='MS')\n",
    "    sensor1 = np.linspace(20, 30, len(date))\n",
    "    sensor2 = np.linspace(15, 25, len(date))\n",
    "    df_temperature_readings = pd.DataFrame({\n",
    "    'Sensor1': sensor1,\n",
    "    'Sensor2': sensor2}, index=date)\n",
    "    print(df_temperature_readings)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_temperature_readings\": df_temperature_readings}\n",
    "\n",
    "pandas_116()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sensor1    Sensor2\n",
      "2023-01-01  20.000000  15.000000\n",
      "2023-02-01  20.909091  15.909091\n",
      "2023-03-01  21.818182  16.818182\n",
      "2023-04-01  22.727273  17.727273\n",
      "2023-05-01  23.636364  18.636364\n",
      "2023-06-01  24.545455  19.545455\n",
      "2023-07-01  25.454545  20.454545\n",
      "2023-08-01  26.363636  21.363636\n",
      "2023-09-01  27.272727  22.272727\n",
      "2023-10-01  28.181818  23.181818\n",
      "2023-11-01  29.090909  24.090909\n",
      "2023-12-01  30.000000  25.000000\n",
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:31:58.338315Z",
     "start_time": "2024-11-11T14:31:58.321954Z"
    }
   },
   "source": [
    "# Use DataFrame `df_sales_tiers` to add a 'SalesTier' column assigned by binning the 'Sales' column into 'Low', 'Medium', and 'High' categories based on [0, 5000, 10000, 15000] bins, storing as `tiered_sales`.\n",
    "@check_pandas_117\n",
    "def pandas_117(df_sales_tiers):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_sales_tiers)\n",
    "    bins = [0, 5000, 10000, 15000]\n",
    "    labels = ['Low', 'Medium', 'High']\n",
    "    df_sales_tiers['SalesTier'] = pd.cut(df_sales_tiers['Sales'], bins=bins, labels=labels)\n",
    "    tiered_sales = df_sales_tiers.copy()\n",
    "    print(tiered_sales)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"tiered_sales\": tiered_sales}\n",
    "\n",
    "pandas_117()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sales SalesTier\n",
      "0   1000       Low\n",
      "1   6000    Medium\n",
      "2  12000      High\n",
      "   Sales SalesTier\n",
      "0   1000       Low\n",
      "1   6000    Medium\n",
      "2  12000      High\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 215
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:39:21.138275Z",
     "start_time": "2024-11-11T14:39:21.082340Z"
    }
   },
   "source": [
    "# Extract month at index of DataFrame `df_time_based_events`, storing them as a new column 'EventMonth' in the DataFrame.\n",
    "@check_pandas_118\n",
    "def pandas_118(df_time_based_events):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_time_based_events)\n",
    "    df_time_based_events['EventMonth'] = df_time_based_events.index.month\n",
    "    print(df_time_based_events)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_time_based_events\": df_time_based_events}\n",
    "\n",
    "pandas_118()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            EventDate\n",
      "2023-01-31 2023-01-31\n",
      "2023-02-28 2023-02-28\n",
      "2023-03-31 2023-03-31\n",
      "            EventDate  EventMonth\n",
      "2023-01-31 2023-01-31           1\n",
      "2023-02-28 2023-02-28           2\n",
      "2023-03-31 2023-03-31           3\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:42:42.997355Z",
     "start_time": "2024-11-11T14:42:42.980125Z"
    }
   },
   "source": [
    "# Given a DataFrame `df_tennis_matches`, select rows using .loc where 'MatchesPlayed' > 20 and store them as `consistent_players`.\n",
    "@check_pandas_119\n",
    "def pandas_119(df_tennis_matches):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_tennis_matches)\n",
    "    consistent_players = df_tennis_matches.loc[df_tennis_matches['MatchesPlayed'] > 20]\n",
    "    print(consistent_players)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"consistent_players\": consistent_players}\n",
    "\n",
    "pandas_119()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Player  MatchesPlayed\n",
      "0      A             10\n",
      "1      B             30\n",
      "2      C             20\n",
      "  Player  MatchesPlayed\n",
      "1      B             30\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:44:58.278308Z",
     "start_time": "2024-11-11T14:44:58.252323Z"
    }
   },
   "source": [
    "# Utilize .iloc on DataFrame `df_daily_results` to select rows by integer location, focusing on every third row, and storing result as `every_third_result`.\n",
    "@check_pandas_120\n",
    "def pandas_120(df_daily_results):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_daily_results)\n",
    "    every_third_result = df_daily_results.iloc[::3]\n",
    "    print(every_third_result)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"every_third_result\": every_third_result}\n",
    "\n",
    "pandas_120()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Result\n",
      "0 2023-01-01    Win\n",
      "1 2023-01-02   Loss\n",
      "2 2023-01-03    Win\n",
      "3 2023-01-04   Loss\n",
      "4 2023-01-05    Win\n",
      "        Date Result\n",
      "0 2023-01-01    Win\n",
      "3 2023-01-04   Loss\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:49:38.260854Z",
     "start_time": "2024-11-11T14:49:38.247067Z"
    }
   },
   "source": [
    "# From DataFrame `df_survey_responses`, combine 'Question' and 'Answer' columns into a single 'Response' column, storing the result as a Series `consolidated_responses` in the format 'Question: Answer'.\n",
    "@check_pandas_121\n",
    "def pandas_121(df_survey_responses):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_survey_responses)\n",
    "    consolidated_responses = df_survey_responses['Question'] + \": \" + df_survey_responses['Answer']\n",
    "    print(consolidated_responses)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"consolidated_responses\": consolidated_responses}\n",
    "\n",
    "pandas_121()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Question Answer\n",
      "0       Q1      A\n",
      "1       Q2      B\n",
      "2       Q3      C\n",
      "0    Q1: A\n",
      "1    Q2: B\n",
      "2    Q3: C\n",
      "dtype: object\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:57:17.575354Z",
     "start_time": "2024-11-11T14:57:17.560604Z"
    }
   },
   "source": [
    "# Use DataFrame `df_server_logs` to group by 'ServerID' and calculate the sum and max of 'ResponseTime', storing result as `server_response_summary`.\n",
    "@check_pandas_122\n",
    "def pandas_122(df_server_logs):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_server_logs)\n",
    "    server_response_summary = df_server_logs.groupby('ServerID')['ResponseTime'].agg(['sum', 'max'])\n",
    "    print(server_response_summary)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"server_response_summary\": server_response_summary}\n",
    "\n",
    "pandas_122()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ServerID  ResponseTime\n",
      "0         1           100\n",
      "1         1           200\n",
      "2         2           300\n",
      "3         2           400\n",
      "          sum  max\n",
      "ServerID          \n",
      "1         300  200\n",
      "2         700  400\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:59:36.340143Z",
     "start_time": "2024-11-11T14:59:36.272838Z"
    }
   },
   "source": [
    "# Modify DataFrame `df_machine_operating` by adding column 'Downtime' calculated from 'EndTime' minus 'StartTime', storing as `operations_with_downtime`.\n",
    "@check_pandas_123\n",
    "def pandas_123(df_machine_operating):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_machine_operating)\n",
    "    df_machine_operating['Downtime'] = df_machine_operating['EndTime'] - df_machine_operating['StartTime']\n",
    "    operations_with_downtime = df_machine_operating.copy()\n",
    "    print(operations_with_downtime)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"operations_with_downtime\": operations_with_downtime}\n",
    "\n",
    "pandas_123()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StartTime    EndTime\n",
      "0 2023-01-01 2023-01-02\n",
      "1 2023-01-02 2023-01-03\n",
      "2 2023-01-03 2023-01-04\n",
      "   StartTime    EndTime Downtime\n",
      "0 2023-01-01 2023-01-02   1 days\n",
      "1 2023-01-02 2023-01-03   1 days\n",
      "2 2023-01-03 2023-01-04   1 days\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:04:43.350816Z",
     "start_time": "2024-11-11T15:04:43.248614Z"
    }
   },
   "source": [
    "# Remove duplicate rows from DataFrame `df_patient_visits`, considering only 'PatientID' and 'VisitDate', saving unique rows as `unique_patient_visits`.\n",
    "@check_pandas_124\n",
    "def pandas_124(df_patient_visits):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_patient_visits)\n",
    "    df_patient_visits.drop_duplicates(subset=['PatientID', 'VisitDate'], keep='first', inplace=True)\n",
    "    unique_patient_visits = df_patient_visits.copy()\n",
    "    print(unique_patient_visits)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"unique_patient_visits\": unique_patient_visits}\n",
    "\n",
    "pandas_124()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PatientID   VisitDate\n",
      "0          1  2023-01-01\n",
      "1          2  2023-01-02\n",
      "2          1  2023-01-01\n",
      "   PatientID   VisitDate\n",
      "0          1  2023-01-01\n",
      "1          2  2023-01-02\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:10:28.892201Z",
     "start_time": "2024-11-11T15:10:28.812876Z"
    }
   },
   "source": [
    "# Optimize data processing speed in DataFrame `df_satellite_data` by converting all text-based columns to category type, storing efficient version as `satellite_optimized`.\n",
    "@check_pandas_125\n",
    "def pandas_125(df_satellite_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_satellite_data)\n",
    "    text_columns = df_satellite_data.select_dtypes(include='object').columns\n",
    "    df_satellite_data[text_columns] = df_satellite_data[text_columns].astype('category')\n",
    "    satellite_optimized = df_satellite_data.copy()\n",
    "    print(satellite_optimized)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"satellite_optimized\": satellite_optimized}\n",
    "\n",
    "pandas_125()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SatelliteID SatelliteName\n",
      "0            1             A\n",
      "1            2             B\n",
      "   SatelliteID SatelliteName\n",
      "0            1             A\n",
      "1            2             B\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:14:22.765459Z",
     "start_time": "2024-11-11T15:14:22.647091Z"
    }
   },
   "source": [
    "# Use vectorized operations on DataFrame `df_temperature_logs` to convert 'TemperatureC' to Fahrenheit, storing the result in `temperature_fahrenheit`.\n",
    "@check_pandas_126\n",
    "def pandas_126(df_temperature_logs):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_temperature_logs)\n",
    "    temperature_fahrenheit = df_temperature_logs['TemperatureC'] * 9/5 + 32\n",
    "    print(temperature_fahrenheit)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"temperature_fahrenheit\": temperature_fahrenheit}\n",
    "\n",
    "pandas_126()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TemperatureC\n",
      "0             0\n",
      "1            10\n",
      "2            20\n",
      "0    32.0\n",
      "1    50.0\n",
      "2    68.0\n",
      "Name: TemperatureC, dtype: float64\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:21:32.061579Z",
     "start_time": "2024-11-11T15:21:32.044683Z"
    }
   },
   "source": [
    "# Group DataFrame `df_streaming_data` by 'UserID' and apply a lambda function to compute total view time, storing the result as `total_view_time`.\n",
    "@check_pandas_127\n",
    "def pandas_127(df_streaming_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_streaming_data)\n",
    "    total_view_time = df_streaming_data.groupby('UserID')['ViewTime'].sum()\n",
    "    print(total_view_time)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"total_view_time\": total_view_time}\n",
    "\n",
    "pandas_127()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  ViewTime\n",
      "0       1       100\n",
      "1       1       200\n",
      "2       2       300\n",
      "3       2       400\n",
      "UserID\n",
      "1    300\n",
      "2    700\n",
      "Name: ViewTime, dtype: int64\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:22:39.179114Z",
     "start_time": "2024-11-11T15:22:39.162049Z"
    }
   },
   "source": [
    "# Use DataFrame `df_sales_analytics` to implement cohort analysis, calculating first purchase date and cohort index, storing result in `sales_cohorts`.\n",
    "@check_pandas_128\n",
    "def pandas_128(df_sales_analytics):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_sales_analytics)\n",
    "    \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sales_cohorts\": sales_cohorts}\n",
    "\n",
    "pandas_128()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID PurchaseDate\n",
      "0       1   2023-01-01\n",
      "1       2   2023-01-02\n",
      "2       1   2023-01-01\n",
      "3       2   2023-01-02\n",
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:34:22.030458Z",
     "start_time": "2024-11-11T15:34:21.892461Z"
    }
   },
   "source": [
    "# In DataFrame `df_web_traffic`, apply conversion to 'Date' column from string to datetime, saving the updated DataFrame as `web_traffic_dates`.\n",
    "@check_pandas_129\n",
    "def pandas_129(df_web_traffic):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_web_traffic)\n",
    "    df_web_traffic['Date'] = pd.to_datetime(df_web_traffic['Date'])\n",
    "    web_traffic_dates = df_web_traffic.copy()\n",
    "    print(web_traffic_dates)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"web_traffic_dates\": web_traffic_dates}\n",
    "\n",
    "pandas_129()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date\n",
      "0  2023-01-01\n",
      "1  2023-01-02\n",
      "        Date\n",
      "0 2023-01-01\n",
      "1 2023-01-02\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:48:24.794352Z",
     "start_time": "2024-11-11T15:48:24.772466Z"
    }
   },
   "source": [
    "# Create DataFrame `df_energy_savings` with datetime index representing bi-weekly dates over one year, filled with repetitive 5% and 10% energy savings, stored in 'SavingsPercent'.\n",
    "@check_pandas_130\n",
    "def pandas_130():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    date_index = pd.date_range(start='2023-01-01', end='2024-01-01', freq='2W')\n",
    "    savings_percent = [0.05, 0.1] * ((len(date_index) + 1) // 2)  \n",
    "    savings_percent = savings_percent[:len(date_index)] \n",
    "    df_energy_savings = pd.DataFrame({'SavingsPercent': savings_percent}, index=date_index)\n",
    "    print(df_energy_savings)\n",
    "    # ABOVE GOES YOUR CODE\\\n",
    "    return {\"df_energy_savings\": df_energy_savings}\n",
    "\n",
    "pandas_130()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SavingsPercent\n",
      "2023-01-01            0.05\n",
      "2023-01-15            0.10\n",
      "2023-01-29            0.05\n",
      "2023-02-12            0.10\n",
      "2023-02-26            0.05\n",
      "2023-03-12            0.10\n",
      "2023-03-26            0.05\n",
      "2023-04-09            0.10\n",
      "2023-04-23            0.05\n",
      "2023-05-07            0.10\n",
      "2023-05-21            0.05\n",
      "2023-06-04            0.10\n",
      "2023-06-18            0.05\n",
      "2023-07-02            0.10\n",
      "2023-07-16            0.05\n",
      "2023-07-30            0.10\n",
      "2023-08-13            0.05\n",
      "2023-08-27            0.10\n",
      "2023-09-10            0.05\n",
      "2023-09-24            0.10\n",
      "2023-10-08            0.05\n",
      "2023-10-22            0.10\n",
      "2023-11-05            0.05\n",
      "2023-11-19            0.10\n",
      "2023-12-03            0.05\n",
      "2023-12-17            0.10\n",
      "2023-12-31            0.05\n",
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:51:54.493396Z",
     "start_time": "2024-11-11T15:51:54.303061Z"
    }
   },
   "source": [
    "# With DataFrame `df_vehicle_data`, apply method chaining to filter 'Type' as 'SUV' and sort by 'RecallDate', saving sorted SUVs as `sorted_suvs`.\n",
    "@check_pandas_131\n",
    "def pandas_131(df_vehicle_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_vehicle_data)\n",
    "    sorted_suvs = df_vehicle_data.loc[df_vehicle_data['Type'] == 'SUV'].sort_values(by='RecallDate')\n",
    "    print(sorted_suvs)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sorted_suvs\": sorted_suvs}\n",
    "\n",
    "pandas_131()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Type  RecallDate\n",
      "0    SUV  2023-01-01\n",
      "1  Sedan  2023-01-02\n",
      "  Type  RecallDate\n",
      "0  SUV  2023-01-01\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T15:54:43.562164Z",
     "start_time": "2024-11-11T15:54:43.480298Z"
    }
   },
   "source": [
    "# Extract 'year' from 'DateAdmitted' in DataFrame `df_patient_admissions` using datetime operations, storing extracted years in `admission_years`.\n",
    "@check_pandas_132\n",
    "def pandas_132(df_patient_admissions):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_patient_admissions)\n",
    "    admission_years = df_patient_admissions['DateAdmitted'].dt.year\n",
    "    print(admission_years)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"admission_years\": admission_years}\n",
    "\n",
    "pandas_132()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DateAdmitted\n",
      "0   2023-01-01\n",
      "1   2023-01-02\n",
      "2   2023-01-03\n",
      "0    2023\n",
      "1    2023\n",
      "2    2023\n",
      "Name: DateAdmitted, dtype: int32\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:06:34.693635Z",
     "start_time": "2024-11-11T16:06:34.675099Z"
    }
   },
   "source": [
    "# Process DataFrame `df_cost_analysis` by converting 'Amount' to USD using 'Currency' which contains the type like ['EUR', 'GBP'] conversion rates being EUR: 1.12 and GBP: 1.30, storing result as `cost_analysis_usd` with column 'Amount' in 'Currency' and 'AmountUSD' in USD.\n",
    "@check_pandas_133\n",
    "def pandas_133(df_cost_analysis):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_cost_analysis)\n",
    "    conversion_rates = {'EUR': 1.12, 'GBP': 1.30} \n",
    "    df_cost_analysis['Amount'] = df_cost_analysis.apply(\n",
    "        lambda row: row['Amount'] * conversion_rates.get(row['Currency']), axis=1)\n",
    "    cost_analysis_usd = df_cost_analysis.copy()\n",
    "    print(cost_analysis_usd)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"cost_analysis_usd\": cost_analysis_usd}\n",
    "\n",
    "pandas_133()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Currency  Amount\n",
      "0      EUR     100\n",
      "1      GBP     200\n",
      "  Currency  Amount\n",
      "0      EUR   112.0\n",
      "1      GBP   260.0\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:10:58.063227Z",
     "start_time": "2024-11-11T16:10:58.045587Z"
    }
   },
   "source": [
    "# In DataFrame `df_logistics_data`, use rolling window of 30 days to calculate moving average inventory level, storing as `thirty_day_moving_inventory`.\n",
    "@check_pandas_134\n",
    "def pandas_134(df_logistics_data):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_logistics_data)\n",
    "    thirty_day_moving_inventory = df_logistics_data['InventoryLevel'].rolling(window=30).mean()\n",
    "    print(thirty_day_moving_inventory)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"thirty_day_moving_inventory\": thirty_day_moving_inventory}\n",
    "\n",
    "pandas_134()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  InventoryLevel\n",
      "0 2023-01-01             100\n",
      "1 2023-01-02             200\n",
      "2 2023-01-03             300\n",
      "3 2023-01-04             400\n",
      "4 2023-01-05             500\n",
      "0   NaN\n",
      "1   NaN\n",
      "2   NaN\n",
      "3   NaN\n",
      "4   NaN\n",
      "Name: InventoryLevel, dtype: float64\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:13:56.257222Z",
     "start_time": "2024-11-11T16:13:55.886580Z"
    }
   },
   "source": [
    "# From DataFrame `df_sports_results`, extract top three teams based on 'Scores', sorting first, saving top teams as `top_teams`.\n",
    "@check_pandas_135\n",
    "def pandas_135(df_sports_results):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_sports_results)\n",
    "    df_sports_results.sort_values(by='Scores', ascending=False, inplace=True)\n",
    "    top_teams = df_sports_results.head(3)\n",
    "    print(top_teams)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"top_teams\": top_teams}\n",
    "\n",
    "pandas_135()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team  Scores\n",
      "0    A     100\n",
      "1    B     200\n",
      "2    C     300\n",
      "  Team  Scores\n",
      "2    C     300\n",
      "1    B     200\n",
      "0    A     100\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:23:30.153903Z",
     "start_time": "2024-11-12T10:23:30.086662Z"
    }
   },
   "source": [
    "\n",
    "# Use DataFrame `df_network_activity` statistics to calculate z-scores for 'Bandwidth', storing in column 'ZscoreBandwidth' as `network_with_zscore`.\n",
    "@check_pandas_136\n",
    "def pandas_136(df_network_activity):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_network_activity)\n",
    "    \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"network_with_zscore\": network_with_zscore}\n",
    "\n",
    "pandas_136()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Bandwidth\n",
      "0        100\n",
      "1        200\n",
      "2        300\n",
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:26:12.524090Z",
     "start_time": "2024-11-12T10:26:12.424068Z"
    }
   },
   "source": [
    "# Perform inplace boolean update of `df_market_responses` setting all 'ResponseTime' > 300 to 300, saving updated DataFrame.\n",
    "@check_pandas_137\n",
    "def pandas_137(df_market_responses):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_market_responses)\n",
    "    df_market_responses.loc[df_market_responses['ResponseTime'] > 300, 'ResponseTime'] = 300\n",
    "    print(df_market_responses)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_market_responses\": df_market_responses}\n",
    "\n",
    "pandas_137()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseTime\n",
      "0           100\n",
      "1           400\n",
      "   ResponseTime\n",
      "0           100\n",
      "1           300\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:32:06.768502Z",
     "start_time": "2024-11-12T10:32:05.656937Z"
    }
   },
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Analyze DataFrame `df_social_behaviors` applying transform with a custom function to 'EngagementRate' to normalize within each 'Group' by z-score, storing as 'NormalizedEngagementRate'.\n",
    "@check_pandas_138\n",
    "def pandas_138(df_social_behaviors):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_social_behaviors)\n",
    "    df_social_behaviors['NormalizedEngagementRate'] = df_social_behaviors.groupby('Group')['EngagementRate'].transform(zscore)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"df_social_behaviors\": df_social_behaviors}\n",
    "\n",
    "pandas_138()"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _fblas: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m zscore\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Analyze DataFrame `df_social_behaviors` applying transform with a custom function to 'EngagementRate' to normalize within each 'Group' by z-score, storing as 'NormalizedEngagementRate'.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;129m@check_pandas_138\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpandas_138\u001B[39m(df_social_behaviors):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# This line is mandatory and should not be removed.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\stats\\__init__.py:610\u001B[0m\n\u001B[0;32m    608\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_morestats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multicomp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m--> 610\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_binomtest\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m binomtest\n\u001B[0;32m    611\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_binned_statistic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    612\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_kde\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gaussian_kde\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:37\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m array, asarray, ma\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspatial\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistance\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cdist\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mndimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _measurements\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_lib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_util\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (check_random_state, MapWrapper, _get_nan,\n\u001B[0;32m     40\u001B[0m                               rng_integers, _rename_parameter, _contains_nan,\n\u001B[0;32m     41\u001B[0m                               AxisError)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\__init__.py:134\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m PytestTester\n\u001B[0;32m    112\u001B[0m submodules \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconstants\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstats\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    131\u001B[0m ]\n\u001B[0;32m    133\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m submodules \u001B[38;5;241m+\u001B[39m [\n\u001B[1;32m--> 134\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLowLevelCallable\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshow_config\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    138\u001B[0m ]\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__dir__\u001B[39m():\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m __all__\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:307\u001B[0m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_matrix_io\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    306\u001B[0m \u001B[38;5;66;03m# For backward compatibility with v0.19.\u001B[39;00m\n\u001B[1;32m--> 307\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m csgraph\n\u001B[0;32m    309\u001B[0m \u001B[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001B[39;00m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    311\u001B[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001B[0;32m    312\u001B[0m     lil, sparsetools, sputils\n\u001B[0;32m    313\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:187\u001B[0m\n\u001B[0;32m    158\u001B[0m __docformat__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestructuredtext en\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    160\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconnected_components\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    161\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlaplacian\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    162\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshortest_path\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    184\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsgraph_to_masked\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    185\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNegativeCycleError\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m--> 187\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_laplacian\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m laplacian\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_shortest_path\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    189\u001B[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson, yen,\n\u001B[0;32m    190\u001B[0m     NegativeCycleError\n\u001B[0;32m    191\u001B[0m )\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_traversal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    193\u001B[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001B[0;32m    194\u001B[0m     depth_first_tree, connected_components\n\u001B[0;32m    195\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m issparse\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearOperator\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_sputils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m convert_pydata_sparse_to_scipy, is_pydata_spmatrix\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m###############################################################################\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Graph laplacian\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:129\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m==================================================\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    126\u001B[0m \n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 129\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_isolve\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dsolve\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_interface\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\__init__.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIterative Solvers for Sparse Linear Systems\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#from info import __doc__\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miterative\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mminres\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m minres\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlgmres\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lgmres\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\iterative.py:5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_interface\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearOperator\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_system\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_lapack_funcs\n\u001B[0;32m      7\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbicg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbicgstab\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcgs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgmres\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mqmr\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_atol_rtol\u001B[39m(name, b_norm, atol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m, rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m):\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py:203\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03m====================================\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mLinear algebra (:mod:`scipy.linalg`)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    200\u001B[0m \n\u001B[0;32m    201\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[1;32m--> 203\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_misc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_cythonized_array_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_basic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\linalg\\_misc.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinAlgError\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mblas\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_blas_funcs\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlapack\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_lapack_funcs\n\u001B[0;32m      6\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLinAlgError\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLinAlgWarning\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnorm\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\scipy\\linalg\\blas.py:213\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m\n\u001B[1;32m--> 213\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _fblas\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _cblas\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _fblas: The specified module could not be found."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:21:24.363099Z",
     "start_time": "2024-11-12T11:21:23.916432Z"
    }
   },
   "source": [
    "# Use DataFrame `df_product_launch` to derive 'ResponseRate' from 'Inquiries' divided by 'Sales', multiplied by 100, storing as `launch_responses`.\n",
    "@check_pandas_139\n",
    "def pandas_139(df_product_launch):\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    print(df_product_launch)\n",
    "    df_product_launch['ResponseRate'] = df_product_launch['Inquiries'] / df_product_launch['Sales'] * 100\n",
    "    launch_responses = df_product_launch.copy()\n",
    "    print(launch_responses)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"launch_responses\": launch_responses}\n",
    "\n",
    "pandas_139()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Inquiries  Sales\n",
      "0        100     10\n",
      "1        200     20\n",
      "   Inquiries  Sales  ResponseRate\n",
      "0        100     10        1000.0\n",
      "1        200     20        1000.0\n",
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:25:27.209579Z",
     "start_time": "2024-11-12T11:25:26.865097Z"
    }
   },
   "source": [
    "# Create a function `create_series_from_dict` that takes a dictionary `input_dict` as an argument, and returns a pandas Series with the dictionary keys as the Series index.\n",
    "@check_pandas_140\n",
    "def pandas_3():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def create_series_from_dict(input_dict):\n",
    "        return pd.Series(input_dict, index=input_dict.keys())\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"create_series_from_dict\": create_series_from_dict}\n",
    "\n",
    "pandas_3()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:26:58.847982Z",
     "start_time": "2024-11-12T11:26:58.792155Z"
    }
   },
   "source": [
    "# Define a function `filter_series_positive` that takes a pandas Series `data_series` and returns a new Series containing only the elements where the value is positive.\n",
    "@check_pandas_141\n",
    "def pandas_4():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def filter_series_positive(data_series):\n",
    "        return data_series[data_series > 0]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filter_series_positive\": filter_series_positive}\n",
    "\n",
    "pandas_4()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:29:06.238588Z",
     "start_time": "2024-11-12T11:29:06.151034Z"
    }
   },
   "source": [
    "# Develop a function `rename_dataframe_columns` that accepts a DataFrame `df` and a dictionary `column_mapping`, and returns the DataFrame with renamed columns according to the mapping.\n",
    "@check_pandas_142\n",
    "def pandas_142():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def rename_dataframe_columns(df, column_mapping):\n",
    "        return df.rename(columns=column_mapping)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"rename_dataframe_columns\": rename_dataframe_columns}\n",
    "\n",
    "pandas_142()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:30:52.310521Z",
     "start_time": "2024-11-12T11:30:52.095101Z"
    }
   },
   "source": [
    "# Write a function `drop_nan_rows` that takes a DataFrame `input_df` and returns a new DataFrame with all rows containing any NaN values removed.\n",
    "@check_pandas_143\n",
    "def pandas_143():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def drop_nan_rows(input_df):\n",
    "        return input_df.dropna()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"drop_nan_rows\": drop_nan_rows}\n",
    "\n",
    "pandas_143()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:31:44.090180Z",
     "start_time": "2024-11-12T11:31:44.004883Z"
    }
   },
   "source": [
    "# Construct a function `fill_missing_with_mean` which takes a DataFrame `df` and fills missing values in each numeric column with the mean of that column, returning the modified DataFrame.\n",
    "@check_pandas_144\n",
    "def pandas_144():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def fill_missing_with_mean(df):\n",
    "        return df.fillna(df.mean())\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"fill_missing_with_mean\": fill_missing_with_mean}\n",
    "\n",
    "pandas_144()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:32:39.750433Z",
     "start_time": "2024-11-12T11:32:39.609802Z"
    }
   },
   "source": [
    "# Implement a function `calculate_group_means` that takes a DataFrame `df` and a column name `group_col`, grouping by `group_col`, then returning the mean of each group as a Series.\n",
    "@check_pandas_145\n",
    "def pandas_145():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_group_means(df, group_col):\n",
    "        return df.groupby(group_col).mean()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_group_means\": calculate_group_means}\n",
    "\n",
    "pandas_145()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:33:36.037542Z",
     "start_time": "2024-11-12T11:33:36.007019Z"
    }
   },
   "source": [
    "# Design a function `subset_dataframe_label` that accepts a DataFrame `df`, a list `rows` of row labels, and returns a subset DataFrame containing only those rows.\n",
    "@check_pandas_146\n",
    "def pandas_146():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def subset_dataframe_label(df, rows):\n",
    "        return df.iloc[rows]\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"subset_dataframe_label\": subset_dataframe_label}\n",
    "\n",
    "pandas_146()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:35:02.836409Z",
     "start_time": "2024-11-12T11:35:02.736082Z"
    }
   },
   "source": [
    "# Define a function `merge_dataframes_on_key` that takes two DataFrames `df1`, `df2`, and a string `key`, merging them on the shared key column and returning the merged DataFrame.\n",
    "@check_pandas_147\n",
    "def pandas_147():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def merge_dataframes_on_key(df1, df2, key):\n",
    "        return pd.merge(df1, df2, on=key)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"merge_dataframes_on_key\": merge_dataframes_on_key}\n",
    "\n",
    "pandas_147()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:41:29.276711Z",
     "start_time": "2024-11-12T11:41:29.252074Z"
    }
   },
   "source": [
    "# Write a function `aggregate_sales_by_region` which accepts a DataFrame `sales_df` that includes columns 'Region' and 'Sales', and returns a DataFrame with the total sales per region.\n",
    "@check_pandas_148\n",
    "def pandas_148():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def aggregate_sales_by_region(sales_df):\n",
    "        return sales_df.groupby('Region')['Sales'].sum()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"aggregate_sales_by_region\": aggregate_sales_by_region}\n",
    "\n",
    "pandas_148()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:40:45.732707Z",
     "start_time": "2024-11-12T11:40:45.259259Z"
    }
   },
   "source": [
    "# Implement a function `pivot_table_for_analysis` that takes a DataFrame `df` and strings `index`, `columns`, `values`, and returns a pivot table using those specifications.\n",
    "@check_pandas_149\n",
    "def pandas_149():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def pivot_table_for_analysis(df, index, columns, values):\n",
    "        return df.pivot_table(index=index, values=values, columns=columns)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"pivot_table_for_analysis\": pivot_table_for_analysis}\n",
    "\n",
    "pandas_149()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:47:07.942445Z",
     "start_time": "2024-11-12T11:47:07.912610Z"
    }
   },
   "source": [
    "# Create a function `calculate_rolling_average` that accepts a DataFrame `df` and an integer `window` to compute the rolling average of a numeric column `column_name`, returning the updated series.\n",
    "@check_pandas_150\n",
    "def pandas_150():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_rolling_average(df, window):\n",
    "        return df.rolling(window=window).mean()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_rolling_average\": calculate_rolling_average}\n",
    "\n",
    "pandas_150()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:46:28.340455Z",
     "start_time": "2024-11-12T11:46:28.288341Z"
    }
   },
   "source": [
    "# Construct a function `resample_time_series` which takes a time-indexed DataFrame `time_df` and a frequency string `freq`, resampling the DataFrame to the new frequency and summing, returning the result.\n",
    "@check_pandas_151\n",
    "def pandas_151():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def resample_time_series(time_df, freq):\n",
    "        return time_df.resample(freq).sum()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"resample_time_series\": resample_time_series}\n",
    "\n",
    "pandas_151()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:49:33.772155Z",
     "start_time": "2024-11-12T11:49:33.708277Z"
    }
   },
   "source": [
    "# Write a function `expand_string_columns` that takes a DataFrame `df` and a list of columns `string_columns`, expanding each string column to lowercase, and returns the modified DataFrame.\n",
    "@check_pandas_152\n",
    "def pandas_152():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def expand_string_columns(df, string_columns):\n",
    "        return df.rename(columns=string_columns).str.lower()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"expand_string_columns\": expand_string_columns}\n",
    "\n",
    "pandas_152()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:02.511284Z",
     "start_time": "2024-11-11T14:30:02.502719Z"
    }
   },
   "source": [
    "# Design a function `convert_to_datetime_index` that accepts a DataFrame `df` and a column name `date_col`, converting it to a DateTimeIndex, and returning the updated DataFrame.\n",
    "@check_pandas_153\n",
    "def pandas_153():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def convert_to_datetime_index(df, date_col):\n",
    "        \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"convert_to_datetime_index\": convert_to_datetime_index}\n",
    "\n",
    "pandas_153()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:02.674641Z",
     "start_time": "2024-11-11T14:30:02.668999Z"
    }
   },
   "source": [
    "# Implement a function `identify_and_remove_outliers` that identifies outliers in a Series `data_series` using a specified `threshold` by removing the values outside of the `threshodl` multiplied by the standard deviation from the mean.\n",
    "@check_pandas_154\n",
    "def pandas_154():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def identify_and_remove_outliers(data_series):\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"identify_and_remove_outliers\": identify_and_remove_outliers}\n",
    "\n",
    "pandas_154()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:10:22.376575Z",
     "start_time": "2024-11-12T13:10:22.211622Z"
    }
   },
   "source": [
    "# Create a function `calculate_percentage_change` which computes the percentage change over time for a Series `time_series` and returns the updated Series with NaN for the first entry.\n",
    "@check_pandas_155\n",
    "def pandas_155():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_percentage_change(time_series):\n",
    "        return time_series.pct_change()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_percentage_change\": calculate_percentage_change}\n",
    "\n",
    "pandas_155()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:02.864355Z",
     "start_time": "2024-11-11T14:30:02.857125Z"
    }
   },
   "source": [
    "# Define a function `categorize_based_on_values` which takes a DataFrame `df`, a column `categorical_col`, and returns a DataFrame with additional column 'Category' based on ranges in `categorical_col` with labels ['Low', 'Medium', 'High'] and bins [0, 100, 200, 300].\n",
    "@check_pandas_156\n",
    "def pandas_156():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def categorize_based_on_values(df, categorical_col):\n",
    "        \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"categorize_based_on_values\": categorize_based_on_values}\n",
    "\n",
    "pandas_156()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:16:57.045677Z",
     "start_time": "2024-11-12T13:16:56.881447Z"
    }
   },
   "source": [
    "# Write a function `shift_dataframe_rows` that takes a DataFrame `df` and an integer `periods`, shifting all rows by the specified number of periods and returning the DataFrame.\n",
    "@check_pandas_157\n",
    "def pandas_157():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def shift_dataframe_rows(df, periods):\n",
    "        return df.shift(periods)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"shift_dataframe_rows\": shift_dataframe_rows}\n",
    "\n",
    "pandas_157()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:03.098496Z",
     "start_time": "2024-11-11T14:30:03.089807Z"
    }
   },
   "source": [
    "# Develop a function `aggregate_and_flatten_grouped` that takes a grouped DataFrame `group_df` and returns a flattened DataFrame with 'sum' and 'count' aggregation for each group and reseted index.\n",
    "@check_pandas_158\n",
    "def pandas_158():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def aggregate_and_flatten_grouped(group_df):\n",
    "        \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"aggregate_and_flatten_grouped\": aggregate_and_flatten_grouped}\n",
    "\n",
    "pandas_158()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:21:33.557578Z",
     "start_time": "2024-11-12T14:21:32.983130Z"
    }
   },
   "source": [
    "# Construct a function named `remove_duplicates_by_columns` that takes a DataFrame `df` and a list `subset_columns`, and removes duplicate rows based on these columns, returning the resulting DataFrame.\n",
    "@check_pandas_159\n",
    "def pandas_159():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def remove_duplicates_by_columns(df, subset_columns):\n",
    "        return df.drop_duplicates(subset=subset_columns)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"remove_duplicates_by_columns\": remove_duplicates_by_columns}\n",
    "\n",
    "pandas_159()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:25:08.842175Z",
     "start_time": "2024-11-12T14:25:08.811814Z"
    }
   },
   "source": [
    "# Design a function `calculate_memory_usage` that takes a DataFrame `df` and returns the total memory usage in MB, both with and without optimizations for all columns possible.\n",
    "@check_pandas_160\n",
    "def pandas_160():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_memory_usage(df):\n",
    "        return df.memory_usage(deep=False, index=True)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_memory_usage\": calculate_memory_usage}\n",
    "\n",
    "pandas_160()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:27:47.852054Z",
     "start_time": "2024-11-12T14:27:47.757846Z"
    }
   },
   "source": [
    "# Implement a function `map_values_with_dict` which accepts a DataFrame `df`, a column `target_col`, and a dictionary `value_map`, returning the updated DataFrame after mapping.\n",
    "@check_pandas_161\n",
    "def pandas_161():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def map_values_with_dict(df, target_col, value_map):\n",
    "        return df.map(value_map)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"map_values_with_dict\": map_values_with_dict}\n",
    "\n",
    "pandas_161()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:30:26.345006Z",
     "start_time": "2024-11-12T14:30:26.002973Z"
    }
   },
   "source": [
    "# Write a function `select_top_n_rows_based_on_column` that takes a DataFrame `df`, a column `target_col`, and an integer `n`, and returns the top `n` rows sorted by `target_col`.\n",
    "@check_pandas_162\n",
    "def pandas_162():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def select_top_n_rows_based_on_column(df, target_col, n):\n",
    "        return df.sort_values(by=target_col, ascending=n).head(n)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"select_top_n_rows_based_on_column\": select_top_n_rows_based_on_column}\n",
    "\n",
    "pandas_162()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:37:25.619982Z",
     "start_time": "2024-11-12T14:37:25.567034Z"
    }
   },
   "source": [
    "# Create a function `replace_substrings_in_column` that takes a DataFrame `df`, a column `text_col`, a string `old`, and a string `new`, replacing all occurrences of `old` with `new` in `text_col`.\n",
    "@check_pandas_163\n",
    "def pandas_163():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def replace_substrings_in_column(df, text_col, old, new):\n",
    "        return df[text_col].replace(old, new)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"replace_substrings_in_column\": replace_substrings_in_column}\n",
    "\n",
    "pandas_163()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:19:13.834475Z",
     "start_time": "2024-11-13T09:19:12.789555Z"
    }
   },
   "source": [
    "# Define a function `apply_discretization_binner` which bins Series `data_series` into a specified number of discrete intervals `n_bins` and returns the binned Series.\n",
    "@check_pandas_164\n",
    "def pandas_164():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def apply_discretization_binner(data_series, n_bins):\n",
    "        return pd.cut(data_series, n_bins)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"apply_discretization_binner\": apply_discretization_binner}\n",
    "\n",
    "pandas_164()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:20:03.318657Z",
     "start_time": "2024-11-13T09:20:01.759108Z"
    }
   },
   "source": [
    "# Write a function `generate_descriptive_statistics` that takes a DataFrame `df` and returns a DataFrame with descriptive statistics such as mean, median, and standard deviation for all numeric columns.\n",
    "@check_pandas_165\n",
    "def pandas_165():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def generate_descriptive_statistics(df):\n",
    "        return df.describe()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"generate_descriptive_statistics\": generate_descriptive_statistics}\n",
    "\n",
    "pandas_165()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:25:28.074465Z",
     "start_time": "2024-11-13T09:25:27.737577Z"
    }
   },
   "source": [
    "# Implement a function `convert_column_dtype` that accepts a DataFrame `df`, a column name `col`, and a data type `new_type`, and returns the DataFrame with `col` converted to `new_type`.\n",
    "@check_pandas_166\n",
    "def pandas_166():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def convert_column_dtype(df, col, new_type):\n",
    "        df[col] = df[col].astype(new_type)\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"convert_column_dtype\": convert_column_dtype}\n",
    "\n",
    "pandas_166()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:30:01.626685Z",
     "start_time": "2024-11-13T09:30:00.819670Z"
    }
   },
   "source": [
    "# Develop a function `sort_dataframe_by_multiple_columns` taking DataFrame `df` and a list `columns_list` to sort the DataFrame by these columns, returning the sorted DataFrame.\n",
    "@check_pandas_167\n",
    "def pandas_167():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def sort_dataframe_by_multiple_columns(df, columns_list):\n",
    "        return df.sort_values(by=columns_list)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sort_dataframe_by_multiple_columns\": sort_dataframe_by_multiple_columns}\n",
    "\n",
    "pandas_167()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T10:28:20.996366Z",
     "start_time": "2024-11-13T10:28:20.667418Z"
    }
   },
   "source": [
    "# Create a function `create_time_based_features` which accepts a DataFrame `time_df` with a 'Timestamp' column and returns the DataFrame with new columns for hour, day, and month.\n",
    "@check_pandas_168\n",
    "def pandas_168():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def create_time_based_features(time_df):\n",
    "        time_df['Timestamp'] = pd.to_datetime(time_df['Timestamp'])\n",
    "        time_df['Hour'] = time_df['Timestamp'].dt.hour\n",
    "        time_df['Day'] = time_df['Timestamp'].dt.day\n",
    "        time_df['Month'] = time_df['Timestamp'].dt.month\n",
    "        return time_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"create_time_based_features\": create_time_based_features}\n",
    "\n",
    "pandas_168()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T11:03:10.352770Z",
     "start_time": "2024-11-13T11:03:09.670982Z"
    }
   },
   "source": [
    "# Create a function `analyze_sales_data` that takes a DataFrame `sales_df` with columns 'Date', 'Region', and 'Sales'. The function should: Convert 'Date' to a DateTime object; Filter out any rows where 'Sales' is negative; Group by 'Region' to calculate the total and average sales; Return a DataFrame with columns 'Region', 'TotalSales', 'AverageSales';\n",
    "@check_pandas_169\n",
    "def pandas_169():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def analyze_sales_data(sales_df):\n",
    "        sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n",
    "        sales_df = sales_df[sales_df['Sales'] >= 0]\n",
    "        result_df = sales_df.groupby('Region')['Sales'].agg(\n",
    "            TotalSales = 'sum', AverageSales = 'mean'\n",
    "        ).reset_index()\n",
    "        return result_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"analyze_sales_data\": analyze_sales_data}\n",
    "\n",
    "pandas_169()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T10:55:30.576534Z",
     "start_time": "2024-11-13T10:55:29.180416Z"
    }
   },
   "source": [
    "# Write a function `clean_and_merge_datasets` that takes two DataFrames `df_left` and `df_right`, and: Fills NA values in `df_left` with 0; Drops any duplicate rows in `df_right`; Merges the cleaned DataFrames on a common column 'Key', using an outer join; Returns the merged DataFrame sorted by 'Key';\n",
    "@check_pandas_170\n",
    "def pandas_170():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def clean_and_merge_datasets(df_left, df_right):\n",
    "        df_left.fillna(0, inplace=True)\n",
    "        df_right.drop_duplicates(inplace=True)\n",
    "        mergedDataFrame = pd.merge(df_left, df_right, on='Key', how='outer')\n",
    "        mergedDataFrame.sort_values(by='Key', inplace=True)\n",
    "        return mergedDataFrame\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"clean_and_merge_datasets\": clean_and_merge_datasets}\n",
    "\n",
    "pandas_170()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:10:04.844117Z",
     "start_time": "2024-11-13T12:10:03.616358Z"
    }
   },
   "source": [
    "# Develop a function `process_sensor_data_batch` that receives a DataFrame `sensor_df` containing 'SensorID', 'ReadingValue', 'Timestamp'. Convert 'Timestamp' to datetime format; Ensure all 'ReadingValue' entries are non-negative; Calculate the mean and standard deviation of 'ReadingValue' for each 'SensorID'; Return a DataFrame with 'SensorID', 'MeanReading', 'StdDevReading';\n",
    "@check_pandas_171\n",
    "def pandas_171():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def process_sensor_data_batch(sensor_df):\n",
    "        sensor_df['Timestamp'] = pd.to_datetime(sensor_df['Timestamp'])\n",
    "        sensor_df = sensor_df[sensor_df['ReadingValue'] >= 0]\n",
    "        result_df = sensor_df.groupby('SensorID')['ReadingValue'].agg(\n",
    "            MeanReading = 'mean', StdDevReading = 'std'\n",
    "        ).reset_index()\n",
    "        return result_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"process_sensor_data_batch\": process_sensor_data_batch}\n",
    "\n",
    "pandas_171()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T07:35:55.525561Z",
     "start_time": "2024-11-15T07:35:55.497452Z"
    }
   },
   "source": [
    "# Construct a function `analyze_customer_behaviors` to handle a DataFrame `customer_df` with columns 'CustomerID', 'PurchaseAmount', 'VisitTimestamp'. Convert 'VisitTimestamp' to a DateTimeIndex; Filter to include only purchases greater than a specified `min_purchase`; Group by 'CustomerID' to determine the total and count of purchases; Return a DataFrame with 'CustomerID', 'TotalPurchases', 'NumberVisits', and attach the most recent visit date;\n",
    "@check_pandas_172\n",
    "def pandas_172():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def analyze_customer_behaviors(customer_df, min_purchase):\n",
    "        customer_df['VisitTimestamp'] = pd.to_datetime(customer_df['VisitTimestamp'])\n",
    "        customer_df.set_index('VisitTimestamp', inplace=True)\n",
    "        customer_df = customer_df[customer_df['PurchaseAmount'] > min_purchase]\n",
    "        result_df = customer_df.groupby('CustomerID').agg(\n",
    "            TotalPurchases=('PurchaseAmount', 'sum'),\n",
    "            NumberVisits=('PurchaseAmount', 'count'),\n",
    "            MostRecentVisit=('VisitTimestamp', 'max')\n",
    "        ).reset_index()\n",
    "        return result_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"analyze_customer_behaviors\": analyze_customer_behaviors}\n",
    "\n",
    "pandas_172()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T07:49:40.937779Z",
     "start_time": "2024-11-15T07:49:40.928752Z"
    }
   },
   "source": [
    "# Create a function `transform_financial_data` that processes a DataFrame `financial_df` including columns 'AccountID', 'TransactionDate', 'Amount'. Parse 'TransactionDate' into datetime and set as index; Filter 'Amount' to exclude NaN and zero values; Extract month and year from 'TransactionDate' into new columns `Month`, `Year`; Group by 'AccountID' and 'Year' to summarize monthly 'Amount' into sum and mean, returning a structured DataFrame;\n",
    "@check_pandas_173\n",
    "def pandas_173():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def transform_financial_data(financial_df):\n",
    "        financial_df['TransactionDate'] = pd.to_datetime(financial_df['TransactionDate'])\n",
    "        financial_df.set_index('TransactionDate', inplace=True)\n",
    "        financial_df = financial_df[financial_df['Amount'].notna() & (financial_df['Amount'] != 0)]\n",
    "        financial_df['Month'] = financial_df.index.month\n",
    "        financial_df['Year'] = financial_df.index.year\n",
    "        result_df = financial_df.groupby(['AccountID', 'Year'])['Amount'].agg(\n",
    "            MonthlyAmountSum='sum',\n",
    "            MonthlyAmountMean='mean'\n",
    "        ).reset_index()\n",
    "        return result_df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"transform_financial_data\": transform_financial_data}\n",
    "\n",
    "pandas_173()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T07:55:01.970348Z",
     "start_time": "2024-11-15T07:55:01.959040Z"
    }
   },
   "source": [
    "# Implement a function `aggregate_weather_data` for a DataFrame `weather_df` with fields 'StationID', 'Temp', 'Humidity', 'ObservationTime'. Convert 'ObservationTime' to a DateTimeIndex; Filter to retain records with positive 'Temp' and 'Humidity'; Resample to daily frequency, taking the mean for each day; Return a DataFrame grouped by 'StationID' with columns for daily average 'Temp' and 'Humidity';\n",
    "@check_pandas_174\n",
    "def pandas_174():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def aggregate_weather_data(weather_df):\n",
    "        weather_df['ObservationTime'] = pd.to_datetime(weather_df['ObservationTime'])\n",
    "        weather_df.set_index('ObservationTime', inplace=True)\n",
    "        \n",
    "        weather_df = weather_df[(weather_df['Temp'] > 0) & (weather_df['Humidity'] > 0)]\n",
    "        daily_weather = weather_df.resample('D').mean()\n",
    "        \n",
    "        result_df = daily_weather.groupby('StationID')[['Temp', 'Humidity']].mean().reset_index()\n",
    "        return result_df\n",
    "        \n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"aggregate_weather_data\": aggregate_weather_data}\n",
    "\n",
    "pandas_174()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.115213Z",
     "start_time": "2024-11-11T14:30:05.104930Z"
    }
   },
   "source": [
    "# Design a function `standardize_student_record` to clean a DataFrame `student_df` featuring 'Name', 'Score', 'SubmissionDate'. Standardize 'Name' to have a capitalized first letter; Address missing 'Score' values by assigning the median score; Standardize 'SubmissionDate' to a consistent format and compute 'DaysSinceSubmission'; Return a DataFrame with standardized names, computed days since submission, and filled scores;\n",
    "@check_pandas_175\n",
    "def pandas_175():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"standardize_student_record\": standardize_student_record}\n",
    "\n",
    "pandas_175()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.181178Z",
     "start_time": "2024-11-11T14:30:05.172233Z"
    }
   },
   "source": [
    "# Write a function `construct_inventory_report` that takes a DataFrame `inventory_df` with 'ItemID', 'Quantity', 'RestockDate'. Parse 'RestockDate' to ensure it's in datetime format; Identify items needing restock by checking 'Quantity' against the `threshold`; Provide a summary count of items needing restock by month; Return a detailed DataFrame with 'ItemID', 'Quantity', 'DaysUntilRestock', plus a monthly summary DataFrame;\n",
    "@check_pandas_176\n",
    "def pandas_176():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"construct_inventory_report\": construct_inventory_report}\n",
    "\n",
    "pandas_176()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.283038Z",
     "start_time": "2024-11-11T14:30:05.274637Z"
    }
   },
   "source": [
    "# Develop a function `optimize_sales_forecast` that works on DataFrame `forecast_df` with columns 'Product', 'ProjectedSales', 'ForecastDate'. Convert 'ForecastDate' to DateTime format; Apply forward fill to handle missing 'ProjectedSales' values; Conduct a rolling window analysis to compute the 3-month moving average of sales; Return an extended DataFrame with moving averages along with original columns;\n",
    "@check_pandas_177\n",
    "def pandas_177():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"optimize_sales_forecast\": optimize_sales_forecast}\n",
    "\n",
    "pandas_177()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.384993Z",
     "start_time": "2024-11-11T14:30:05.371442Z"
    }
   },
   "source": [
    "# Create a function `summarize_employee_performance` that processes DataFrame `performance_df` with 'EmployeeID', 'Score', 'ReviewDate'. Ensure 'ReviewDate' is converted into a datetime object; Replace missing 'Score' with the lowest non-zero score; Group by 'EmployeeID' to compute total and average score; Generate a DataFrame highlighting top performers with scores above a specified percentile score; Audit history, including evaluation of performance improvement over time;\n",
    "@check_pandas_178\n",
    "def pandas_178():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"summarize_employee_performance\": summarize_employee_performance}\n",
    "\n",
    "pandas_178()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.485604Z",
     "start_time": "2024-11-11T14:30:05.478123Z"
    }
   },
   "source": [
    "# Create a function `double_series_values` that takes a Series `input_series` and returns a Series with all values doubled.\n",
    "@check_pandas_179\n",
    "def pandas_179():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"double_series_values\": double_series_values}\n",
    "\n",
    "pandas_179()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 191
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.565344Z",
     "start_time": "2024-11-11T14:30:05.553063Z"
    }
   },
   "source": [
    "# Write a function `replace_zeros_with_mean` that takes a Series `data_series` and replaces all 0 values with the mean of the Series, returning the modified Series.\n",
    "@check_pandas_180\n",
    "def pandas_180():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"replace_zeros_with_mean\": replace_zeros_with_mean}\n",
    "\n",
    "pandas_180()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.659760Z",
     "start_time": "2024-11-11T14:30:05.644068Z"
    }
   },
   "source": [
    "# Design a function `standardize_column_names` that accepts a DataFrame `df` and returns it with all column names set to lowercase.\n",
    "@check_pandas_181\n",
    "def pandas_181():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"standardize_column_names\": standardize_column_names}\n",
    "\n",
    "pandas_181()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 193
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.766797Z",
     "start_time": "2024-11-11T14:30:05.758983Z"
    }
   },
   "source": [
    "# Implement a function `drop_duplicate_rows` that receives a DataFrame `df` and returns it with duplicate rows removed.\n",
    "@check_pandas_182\n",
    "def pandas_182():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"drop_duplicate_rows\": drop_duplicate_rows}\n",
    "\n",
    "pandas_182()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.857928Z",
     "start_time": "2024-11-11T14:30:05.850754Z"
    }
   },
   "source": [
    "# Create a function `calculate_column_sums` that takes a DataFrame `df` and returns a Series containing the sum of each column.\n",
    "@check_pandas_183\n",
    "def pandas_183():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_column_sums\": calculate_column_sums}\n",
    "\n",
    "pandas_183()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 195
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:05.937356Z",
     "start_time": "2024-11-11T14:30:05.919921Z"
    }
   },
   "source": [
    "# Write a function `extract_date_parts` that takes a Series `dates` of datetime objects and returns a DataFrame with columns 'Year', 'Month', and 'Day'.\n",
    "@check_pandas_184\n",
    "def pandas_184():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"extract_date_parts\": extract_date_parts}\n",
    "\n",
    "pandas_184()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.017374Z",
     "start_time": "2024-11-11T14:30:06.003752Z"
    }
   },
   "source": [
    "# Define a function `concat_strings_in_column` that, given a DataFrame `df` and a column `text_col`, concatenates all strings in that column with a space in between, returning the result string.\n",
    "@check_pandas_185\n",
    "def pandas_185():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"concat_strings_in_column\": concat_strings_in_column}\n",
    "\n",
    "pandas_185()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.137697Z",
     "start_time": "2024-11-11T14:30:06.120529Z"
    }
   },
   "source": [
    "# Implement a function `sort_by_index_descending` that accepts a DataFrame `df` and returns it sorted by its index in descending order.\n",
    "@check_pandas_186\n",
    "def pandas_186():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sort_by_index_descending\": sort_by_index_descending}\n",
    "\n",
    "pandas_186()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.211310Z",
     "start_time": "2024-11-11T14:30:06.191468Z"
    }
   },
   "source": [
    "# Create a function `filter_by_threshold` to take a DataFrame `df` and a column name `col`, returning a DataFrame including only rows where `col` is above a given threshold.\n",
    "@check_pandas_187\n",
    "def pandas_187():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filter_by_threshold\": filter_by_threshold}\n",
    "\n",
    "pandas_187()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.294291Z",
     "start_time": "2024-11-11T14:30:06.272356Z"
    }
   },
   "source": [
    "# Write a function `get_unique_values` that takes a DataFrame `df` and a column `col`, returning a sorted array of unique values in the specified column.\n",
    "@check_pandas_188\n",
    "def pandas_188():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"get_unique_values\": get_unique_values}\n",
    "\n",
    "pandas_188()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 200
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.390460Z",
     "start_time": "2024-11-11T14:30:06.371515Z"
    }
   },
   "source": [
    "# Develop a function `append_row_to_dataframe` that takes a DataFrame `df` and a dictionary `row_dict`, appending the dictionary as a new row, and returning the updated DataFrame.\n",
    "@check_pandas_189\n",
    "def pandas_189():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"append_row_to_dataframe\": append_row_to_dataframe}\n",
    "\n",
    "pandas_189()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 201
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.470498Z",
     "start_time": "2024-11-11T14:30:06.457088Z"
    }
   },
   "source": [
    "# Construct a function `reset_index_and_name` that takes a DataFrame `df` and returns it with its index reset and the name of the index set to 'NewIndex'.\n",
    "@check_pandas_190\n",
    "def pandas_190():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"reset_index_and_name\": reset_index_and_name}\n",
    "\n",
    "pandas_190()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.576582Z",
     "start_time": "2024-11-11T14:30:06.565454Z"
    }
   },
   "source": [
    "# Create a function `swap_dataframe_columns` that accepts a DataFrame `df` and two column names `col1` and `col2`, swapping the values of these columns.\n",
    "@check_pandas_191\n",
    "def pandas_191():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"swap_dataframe_columns\": swap_dataframe_columns}\n",
    "\n",
    "pandas_191()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.672578Z",
     "start_time": "2024-11-11T14:30:06.659727Z"
    }
   },
   "source": [
    "# Write a function `rename_index_label` which receives a DataFrame `df`, renames its first index label to 'FirstRow', and returns the DataFrame.\n",
    "@check_pandas_192\n",
    "def pandas_192():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"rename_index_label\": rename_index_label}\n",
    "\n",
    "pandas_192()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.783514Z",
     "start_time": "2024-11-11T14:30:06.771194Z"
    }
   },
   "source": [
    "# Define a function `calculate_frequency_table` that, given a DataFrame `df` and a column `cat_col`, returns a DataFrame with a frequency count of each category with columns Category and count.\n",
    "@check_pandas_193\n",
    "def pandas_193():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_frequency_table\": calculate_frequency_table}\n",
    "\n",
    "pandas_193()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.864918Z",
     "start_time": "2024-11-11T14:30:06.850736Z"
    }
   },
   "source": [
    "# Implement a function `remove_negative_entries` that takes a Series `numeric_series` and returns a Series with all negative entries removed.\n",
    "@check_pandas_194\n",
    "def pandas_194():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"remove_negative_entries\": remove_negative_entries}\n",
    "\n",
    "pandas_194()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 206
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:06.948644Z",
     "start_time": "2024-11-11T14:30:06.941896Z"
    }
   },
   "source": [
    "# Write a function `sort_column_values` that takes a DataFrame `df` and column name `col`, returning `df` sorted by `col` values in ascending order.\n",
    "@check_pandas_195\n",
    "def pandas_195():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"sort_column_values\": sort_column_values}\n",
    "\n",
    "pandas_195()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:07.035322Z",
     "start_time": "2024-11-11T14:30:07.025613Z"
    }
   },
   "source": [
    "# Design a function `drop_columns_by_name` that accepts a DataFrame `df` and a list of column names `drop_cols`, removing these columns and returning the modified DataFrame.\n",
    "@check_pandas_196\n",
    "def pandas_196():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"drop_columns_by_name\": drop_columns_by_name}\n",
    "\n",
    "pandas_196()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 208
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:07.105912Z",
     "start_time": "2024-11-11T14:30:07.086856Z"
    }
   },
   "source": [
    "# Develop a function `strip_whitespace` that takes a DataFrame `df` and trims leading and trailing whitespace from all string entries.\n",
    "@check_pandas_197\n",
    "def pandas_197():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"strip_whitespace\": strip_whitespace}\n",
    "\n",
    "pandas_197()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ The implementation is incorrect or the exercise was not implemented.\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T08:05:06.283183Z",
     "start_time": "2024-11-15T08:05:06.255637Z"
    }
   },
   "source": [
    "# Create a function `duplicate_last_column` that receives a DataFrame `df` and duplicates its last column, appending the duplicate to the right of the DataFrame named as the original column with '_copy' appended.\n",
    "@check_pandas_198\n",
    "def pandas_198():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def duplicate_last_column(df):\n",
    "        last_column = df.columns[-1]\n",
    "        df[f\"{last_column}_copy\"] = df[last_column]\n",
    "        return df\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"duplicate_last_column\": duplicate_last_column}\n",
    "\n",
    "pandas_198()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T07:59:26.833872Z",
     "start_time": "2024-11-15T07:59:26.816670Z"
    }
   },
   "source": [
    "# Write a function `filter_top_n_rows_by_column` that takes a DataFrame `df`, a column `col`, and an integer `n`, returning the top n rows sorted by values in `col`.\n",
    "@check_pandas_199\n",
    "def pandas_199():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def filter_top_n_rows_by_column(df, col, n):\n",
    "        df.sort_values(by=col, ascending=False, inplace=True)\n",
    "        return df.head(n)\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"filter_top_n_rows_by_column\": filter_top_n_rows_by_column}\n",
    "\n",
    "pandas_199()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T07:56:04.655383Z",
     "start_time": "2024-11-15T07:56:04.638528Z"
    }
   },
   "source": [
    "# Define a function `calculate_range_of_numeric_series` that takes a Series `numeric_series` and returns the range (max - min) of its values.\n",
    "@check_pandas_200\n",
    "def pandas_200():\n",
    "    # This line is mandatory and should not be removed.\n",
    "    ex_stat_init = True\n",
    "\n",
    "    # BELOW GOES YOUR CODE\n",
    "    def calculate_range_of_numeric_series(numeric_series):\n",
    "        return numeric_series.max() - numeric_series.min()\n",
    "    # ABOVE GOES YOUR CODE\n",
    "    return {\"calculate_range_of_numeric_series\": calculate_range_of_numeric_series}\n",
    "\n",
    "pandas_200()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Great job! Exercise completed successfully.\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "excalibur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
